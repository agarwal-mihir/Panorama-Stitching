{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUSTOM IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIFT-Based Feature Matching\n",
    "\n",
    "The function `detectFeaturesAndMatch` aims to establish a correspondence between keypoints in two images using Scale-Invariant Feature Transform (SIFT) and Brute-Force Matching. SIFT is an image descriptor for image-based matching and object recognition that is invariant to changes in scale, rotation, and partially invariant to affine distortions and changes in illumination. Mathematically, SIFT descriptors are high-dimensional vectors extracted from a sub-region around each keypoint and are highly distinctive. The formula to calculate the SIFT descriptor involves the gradient magnitude and direction calculated in the keypoint's neighborhood.\n",
    "\n",
    "$\n",
    "\\text{Descriptor} = f\\left(\\frac{\\partial I(x,y)}{\\partial x}, \\frac{\\partial I(x,y)}{\\partial y}\\right)\n",
    "$\n",
    "\n",
    "After SIFT feature extraction, the next step is matching the descriptors from two images. The function utilizes Brute-Force Matcher with L2 norm for this purpose. The Brute-Force matcher takes the descriptor of a feature in the first set and matches it with all other features in the second set using a distance calculation, often the Euclidean distance:\n",
    "\n",
    "$\n",
    "\\text{Distance} = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}\n",
    "$\n",
    "\n",
    "The matches are then sorted by this distance, providing a set of best matches. This approach allows for identifying correspondences between two images, which can be useful in tasks like image stitching, object recognition, and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectFeaturesAndMatch(image1, image2, maxNumOfFeatures=30):\n",
    "    '''\n",
    "    Takes two images as input and returns a set of correspondences between the two images, matched using SIFT features.\n",
    "    The matches are sorted from best to worst based on the Euclidean (L2) distance between feature descriptors.\n",
    "    '''\n",
    "\n",
    "    # Initialize the SIFT feature detector and descriptor\n",
    "    siftDetector = cv2.SIFT_create()\n",
    "\n",
    "    # Detect keypoints and compute descriptors for both images\n",
    "    keypoints1, descriptors1 = siftDetector.detectAndCompute(image1, None)\n",
    "    keypoints2, descriptors2 = siftDetector.detectAndCompute(image2, None)\n",
    "\n",
    "    # Initialize BFMatcher with L2 norm for matching SIFT descriptors\n",
    "    bruteForceMatcher = cv2.BFMatcher(cv2.NORM_L2)\n",
    "\n",
    "    # Perform the matching between descriptors of two images\n",
    "    rawMatches = bruteForceMatcher.match(descriptors1, descriptors2)\n",
    "\n",
    "    # Sort the matches based on their distance attributes\n",
    "    sortedMatches = sorted(rawMatches, key=lambda x: x.distance)\n",
    "\n",
    "    # Initialize an empty list to store corresponding points between two images\n",
    "    featureCorrespondences = []\n",
    "    for match in sortedMatches:\n",
    "        featureCorrespondences.append((keypoints1[match.queryIdx].pt, keypoints2[match.trainIdx].pt))\n",
    "\n",
    "    print(f'Total number of matches: {len(featureCorrespondences)}')\n",
    "\n",
    "    # Prepare source and destination points for homography\n",
    "    sourcePoints = np.float32([point_pair[0] for point_pair in featureCorrespondences[:maxNumOfFeatures]]).reshape(-1, 1, 2)\n",
    "    destinationPoints = np.float32([point_pair[1] for point_pair in featureCorrespondences[:maxNumOfFeatures]]).reshape(-1, 1, 2)\n",
    "\n",
    "    return np.array(featureCorrespondences[:maxNumOfFeatures]), sourcePoints, destinationPoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homography Estimation via Direct Linear Transformation (DLT) and RANSAC\n",
    "\n",
    "The first function, `calculateHomography`, uses Direct Linear Transformation (DLT) to calculate the homography matrix, $H$, from a set of point correspondences. DLT works by transforming the problem into a system of linear equations. The core of DLT involves solving the equation $Ah = 0$, where $A$ is constructed using the coordinates of point correspondences, and $h$ is a vector that contains the flattened homography matrix. Singular Value Decomposition (SVD) is then used to solve this equation system.\n",
    "\n",
    "$\n",
    "A = \\begin{bmatrix}\n",
    "x' & y' & 1 & 0 & 0 & 0 & -x'x & -x'y & -x' \\\\\n",
    "0 & 0 & 0 & x' & y' & 1 & -y'x & -y'y & -y' \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\n",
    "\\end{bmatrix}\n",
    "$\n",
    "$\n",
    "h = [h_{11}, h_{12}, h_{13}, h_{21}, h_{22}, h_{23}, h_{31}, h_{32}, h_{33}]^T\n",
    "$\n",
    "\n",
    "The second function, `getBestHomographyRANSAC`, incorporates the RANSAC (Random Sample Consensus) algorithm to robustly estimate the homography matrix. RANSAC aims to find the model parameters that generate the maximum number of inliers, thereby mitigating the impact of outliers. In each iteration, a random subset of point correspondences is chosen to estimate the homography matrix. Then, other points are tested against this estimated homography to find inliers based on a predefined threshold. The best estimated homography is updated if a trial results in more inliers than any previous iteration.\n",
    "\n",
    "$\n",
    "\\text{Error} = \\lVert \\text{dst\\_point} - \\text{estimated\\_dst} \\rVert_2\n",
    "$\n",
    "\n",
    "Here, $\\lVert \\cdot \\rVert_2$ denotes the L2 norm, and the error is checked against a threshold to classify inliers and outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateHomography(point_correspondences):\n",
    "    '''\n",
    "    Calculate the homography matrix using the Direct Linear Transformation (DLT) algorithm.\n",
    "    Takes in a set of point correspondences and returns the 3x3 homography matrix.\n",
    "\n",
    "    Parameters:\n",
    "        point_correspondences (list): List of matched points between two images. Each element should be a tuple (src_point, dst_point).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The 3x3 homography matrix.\n",
    "    '''\n",
    "  \n",
    "    # Initialize an empty list to construct matrix A, which will be used to solve for the homography\n",
    "    matrix_A = []\n",
    "\n",
    "    for correspondence in point_correspondences:\n",
    "        # Extract source and destination coordinates from each correspondence\n",
    "        src_point, dst_point = correspondence\n",
    "        src_x, src_y = src_point[0], src_point[1]\n",
    "        dst_x, dst_y = dst_point[0], dst_point[1]\n",
    "\n",
    "        # Append the equations to matrix A, following the DLT algorithm\n",
    "        matrix_A.append([src_x, src_y, 1, 0, 0, 0, -dst_x * src_x, -dst_x * src_y, -dst_x])\n",
    "        matrix_A.append([0, 0, 0, src_x, src_y, 1, -dst_y * src_x, -dst_y * src_y, -dst_y])\n",
    "\n",
    "    # Convert the list to a NumPy array for numerical computations\n",
    "    matrix_A = np.asarray(matrix_A)\n",
    "\n",
    "    # Perform Singular Value Decomposition (SVD) to solve for the homography matrix\n",
    "    U, S, Vh = np.linalg.svd(matrix_A)\n",
    "\n",
    "    # Extract the last row of Vh and normalize it to get the homography elements\n",
    "    homography_elements = Vh[-1, :] / Vh[-1, -1]\n",
    "\n",
    "    # Reshape to form the 3x3 homography matrix\n",
    "    homography_matrix = homography_elements.reshape(3, 3)\n",
    "\n",
    "    return homography_matrix  # Return the computed 3x3 homography matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestHomographyRANSAC(correspondences, trials=1000, threshold=10, num_samples=4):\n",
    "    '''\n",
    "    Estimate the homography matrix using the RANSAC algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        correspondences (list): List of point correspondences between two images.\n",
    "        trials (int): Number of RANSAC trials.\n",
    "        threshold (float): RANSAC threshold for determining inliers.\n",
    "        num_samples (int): Number of random samples to choose for each trial.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The estimated 3x3 homography matrix.\n",
    "        list: The random sample of point correspondences that led to the estimated homography.\n",
    "    '''\n",
    "\n",
    "    # Initialize variables to keep track of the best homography and inliers\n",
    "    best_homography = None\n",
    "    max_inliers = []\n",
    "    \n",
    "    # Iterate through RANSAC trials\n",
    "    for i in tqdm(range(trials)):\n",
    "        # Randomly select 'num_samples' correspondences\n",
    "        selected_indices = np.random.choice(len(correspondences), num_samples, replace=False)\n",
    "        random_sample = correspondences[selected_indices]\n",
    "        \n",
    "        # Compute the homography using the random sample\n",
    "        estimated_homography = calculateHomography(random_sample)\n",
    "        \n",
    "        # Initialize list to keep track of inliers for this iteration\n",
    "        current_inliers = []\n",
    "        \n",
    "        for correspondence in correspondences:\n",
    "            # Prepare source and destination points\n",
    "            src_point = np.append(correspondence[0], 1)\n",
    "            dst_point = np.append(correspondence[1], 1)\n",
    "            \n",
    "            # Estimate the destination point based on the current homography\n",
    "            estimated_dst = np.dot(estimated_homography, src_point)\n",
    "            estimated_dst /= estimated_dst[-1]\n",
    "            \n",
    "            # Calculate the error (Euclidean distance)\n",
    "            error = np.linalg.norm(dst_point - estimated_dst)\n",
    "            \n",
    "            # Check if this correspondence is an inlier\n",
    "            if error < threshold:\n",
    "                current_inliers.append(correspondence)\n",
    "        \n",
    "        # Update if this trial gives more inliers than before\n",
    "        if len(current_inliers) > len(max_inliers):\n",
    "            max_inliers = current_inliers\n",
    "            best_homography = estimated_homography\n",
    "\n",
    "    print(f\"Max inliers = {len(max_inliers)}\")\n",
    "    return best_homography, random_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Bounding Box of a Warped Image\n",
    "\n",
    "The function `computeBoundingBoxOfWarpedImage` calculates the bounding box for an image after it undergoes a homographic transformation, defined by a given 3x3 homography matrix $H$. The bounding box is essential for stitching or overlaying the transformed image onto another canvas.\n",
    "\n",
    "To compute the bounding box, we first define the four corner points of the original image in homogeneous coordinates: \\([x, y, 1]^T\\). The four corners are the top-left \\((0, 0)\\), top-right \\((\\text{img\\_width}-1, 0)\\), bottom-left \\((0, \\text{img\\_height}-1)\\), and bottom-right \\((\\text{img\\_width}-1, \\text{img\\_height}-1)\\) points. These points are encapsulated into a 3x4 matrix $P$.\n",
    "\n",
    "$\n",
    "P = \\begin{bmatrix}\n",
    "0 & \\text{img\\_width}-1 & 0 & \\text{img\\_width}-1 \\\\\n",
    "0 & 0 & \\text{img\\_height}-1 & \\text{img\\_height}-1 \\\\\n",
    "1 & 1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "We then apply the homography matrix $H$ to $P$ to find the transformed corner points:\n",
    "\n",
    "$\n",
    "\\text{Transformed\\_corners} = H \\times P\n",
    "$\n",
    "\n",
    "After transforming, the coordinates are normalized to inhomogeneous form by dividing each coordinate by its $w$-component. Finally, the bounding box is determined by finding the minimum and maximum $x$ and $y$ coordinates among the transformed corners.\n",
    "\n",
    "$\n",
    "(x_{\\text{min}}, x_{\\text{max}}, y_{\\text{min}}, y_{\\text{max}}) = (\\min(x), \\max(x), \\min(y), \\max(y))\n",
    "$\n",
    "\n",
    "This bounding box is crucial for understanding the spatial requirements when integrating the transformed image with other images or backgrounds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeBoundingBoxOfWarpedImage(homography_matrix, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Compute the bounding box of an image after it has been transformed by a given homography matrix.\n",
    "\n",
    "    Args:\n",
    "        homography_matrix (np.ndarray): 3x3 matrix used for the homographic transformation.\n",
    "        img_width (int): Width of the original image.\n",
    "        img_height (int): Height of the original image.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Coordinates of the bounding box as (x_min, x_max, y_min, y_max).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define corners of the original image [Top-left, Top-right, Bottom-left, Bottom-right]\n",
    "    # Each column is a corner point in homogeneous coordinates (x, y, 1)\n",
    "    original_corners = np.array([[0, img_width - 1, 0, img_width - 1], \n",
    "                                 [0, 0, img_height - 1, img_height - 1], \n",
    "                                 [1, 1, 1, 1]])\n",
    "    \n",
    "    # Apply the homography to the original corners\n",
    "    transformed_corners = np.dot(homography_matrix, original_corners)\n",
    "    \n",
    "    # Convert to inhomogeneous coordinates by dividing by the third (w) component\n",
    "    transformed_corners /= transformed_corners[2, :]\n",
    "    \n",
    "    # Find the minimum and maximum x and y coordinates of the transformed corners\n",
    "    x_min = np.min(transformed_corners[0])\n",
    "    x_max = np.max(transformed_corners[0])\n",
    "    y_min = np.min(transformed_corners[1])\n",
    "    y_max = np.max(transformed_corners[1])\n",
    "    \n",
    "    return int(x_min), int(x_max), int(y_min), int(y_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `warpAndPlaceSourceImage` Function Analysis\n",
    "\n",
    "The function `warpAndPlaceSourceImage` performs the task of warping a source image based on a given 3x3 homography matrix $H$ and places it onto a destination image. The function offers flexibility by allowing both forward and inverse mapping techniques and also allows for a positional offset.\n",
    "\n",
    "## Forward Mapping Method\n",
    "\n",
    "In the forward mapping method, the homography matrix $H$ is directly applied to each coordinate of the source image. This approach is computationally expedient but can lead to holes or gaps in the destination image due to mapping conflicts (multiple source pixels mapping to the same destination pixel) or an absence of mapping (some destination pixels receiving no mapping).\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Generate Indices**: Create a grid of `(x, y)` coordinates that span the source image and convert these coordinates to their homogeneous form:\n",
    "\n",
    "    $\n",
    "    \\text{Homogeneous Coords} = \\begin{pmatrix} x \\\\ y \\\\ 1 \\end{pmatrix}\n",
    "    $\n",
    "\n",
    "2. **Apply Homography**: Perform the dot product between $H$ and the homogeneous coordinates to get the transformed coordinates:\n",
    "\n",
    "    $\n",
    "    \\text{Transformed Coords} = H \\times \\text{Homogeneous Coords}\n",
    "    $\n",
    "  \n",
    "3. **Normalize**: Convert to inhomogeneous coordinates by dividing by the $w$-component.\n",
    "  \n",
    "4. **Place onto Destination**: The transformed coordinates are directly indexed into the destination image, and any offset is added at this stage.\n",
    "\n",
    "## Inverse Mapping Method\n",
    "\n",
    "In the inverse mapping method, each pixel in the destination image is traced back to its corresponding pixel in the source image. This method is computationally intensive but it eliminates the issue of holes or gaps by ensuring every destination pixel is adequately mapped.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Compute Bounding Box**: First, use `computeBoundingBoxOfWarpedImage` to find the bounding box that the transformed source image will occupy on the destination image.\n",
    "\n",
    "2. **Generate Indices**: Create a grid of `(x, y)` coordinates that reside within this bounding box in the destination image. Convert these to homogeneous coordinates.\n",
    "  \n",
    "3. **Apply Inverse Homography**: Multiply the homogeneous coordinates by $H^{-1}$ to find the corresponding coordinates in the source image:\n",
    "\n",
    "    $\n",
    "    \\text{Transformed Coords} = H^{-1} \\times \\text{Homogeneous Coords}\n",
    "    $\n",
    "\n",
    "4. **Normalize and Filter**: Convert the coordinates back to their inhomogeneous form and perform boundary checks to ensure the coordinates lie within the dimensions of the source image.\n",
    "\n",
    "5. **Map to Source**: Use the transformed coordinates to fetch the relevant pixels from the source image and map them onto the destination image. Any positional offset is added here.\n",
    "\n",
    "This technique offers a more accurate mapping, making it more suitable for tasks that require a high level of precision such as image stitching or complex scene overlays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warpAndPlaceSourceImage(source_img, homography_matrix, dest_img, use_forward_mapping=False, offset=(0, 0)):\n",
    "    \"\"\"\n",
    "    Warps the source image according to a given homography matrix and places it onto the destination image.\n",
    "\n",
    "    Args:\n",
    "        source_img (np.ndarray): The source image array to be warped.\n",
    "        homography_matrix (np.ndarray): The 3x3 matrix governing the homographic transformation.\n",
    "        dest_img (np.ndarray): The destination image array where the warped source image will be placed.\n",
    "        use_forward_mapping (bool): Determines whether to use forward or inverse mapping for the transformation.\n",
    "        offset (tuple): Offsets for placing the warped image onto the destination image in (x, y) coordinates.\n",
    "\n",
    "    Returns:\n",
    "        None: The destination image is modified in place.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the dimensions of the source image\n",
    "    height, width, _ = source_img.shape\n",
    "    \n",
    "    # Compute the inverse of the homography matrix for inverse mapping\n",
    "    homography_inv = np.linalg.inv(homography_matrix)\n",
    "\n",
    "    if use_forward_mapping:\n",
    "        # FORWARD MAPPING\n",
    "\n",
    "        # Generate indices for source image coordinates\n",
    "        coords = np.indices((width, height)).reshape(2, -1)\n",
    "        homogeneous_coords = np.vstack((coords, np.ones(coords.shape[1])))\n",
    "        \n",
    "        # Apply the homography transformation\n",
    "        transformed_coords = np.dot(homography_matrix, homogeneous_coords)\n",
    "        transformed_coords /= transformed_coords[2, :]\n",
    "        \n",
    "        # Extract the integer coordinates of the output\n",
    "        x_output, y_output = transformed_coords.astype(np.int32)[:2, :]\n",
    "        \n",
    "        # Place the transformed source image onto the destination image\n",
    "        dest_img[y_output + offset[1], x_output + offset[0]] = source_img[coords[1], coords[0]]\n",
    "\n",
    "    else:\n",
    "        # INVERSE MAPPING\n",
    "        \n",
    "        # Compute bounding box of the warped image\n",
    "        x_min, x_max, y_min, y_max = computeBoundingBoxOfWarpedImage(homography_matrix, width, height)\n",
    "        \n",
    "        # Generate indices for destination image coordinates within bounding box\n",
    "        coords = np.indices((x_max - x_min, y_max - y_min)).reshape(2, -1)\n",
    "        coords += np.array([[x_min], [y_min]])\n",
    "        homogeneous_coords = np.vstack((coords, np.ones(coords.shape[1])))\n",
    "        \n",
    "        # Apply the inverse homography transformation\n",
    "        transformed_coords = np.dot(homography_inv, homogeneous_coords)\n",
    "        transformed_coords /= transformed_coords[2, :]\n",
    "        \n",
    "        # Extract the integer coordinates of the input\n",
    "        x_input, y_input = transformed_coords.astype(np.int32)[:2, :]\n",
    "        \n",
    "        # Perform boundary check to make sure coordinates are within source image dimensions\n",
    "        valid_indices = np.where((x_input >= 0) & (x_input < width) & (y_input >= 0) & (y_input < height))\n",
    "\n",
    "        # Map the inverse-transformed points to the source image and place onto the destination image\n",
    "        dest_img[coords[1, valid_indices] + offset[1], coords[0, valid_indices] + offset[0]] = source_img[y_input[valid_indices], x_input[valid_indices]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageBlenderWithPyramids Class Overview\n",
    "\n",
    "The `ImageBlenderWithPyramids` class specializes in image blending using Gaussian and Laplacian pyramids. It is tailored for the 'STRAIGHTCUT' blending strategy. Here, we dissect the intricacies of each of its methods and their computational significance.\n",
    "\n",
    "## Constructor: \n",
    "\n",
    "This initializes the class with the depth of the pyramids. Higher depth allows for more seamless blending but can be computationally intensive.\n",
    "\n",
    "## Gaussian Pyramid: \n",
    "\n",
    "Generates a Gaussian pyramid for a given image. The pyramid is built through recursive down-sampling, primarily used for multi-scale image analysis.\n",
    "\n",
    "## Laplacian Pyramid:\n",
    "\n",
    "Generates a Laplacian pyramid, capturing the \"details\" or high-frequency components of the image. It's constructed from the Gaussian pyramid.\n",
    "\n",
    "## Blended Pyramid: \n",
    "\n",
    "Blends two Laplacian pyramids using a Gaussian pyramid as the mask. It performs a weighted sum of the two pyramids, element-wise.\n",
    "\n",
    "## Image Reconstruction:\n",
    "\n",
    "Given a Laplacian pyramid, this function reconstructs the original image by starting from the smallest (last) image in the pyramid and progressively adding detail back into the image.\n",
    "\n",
    "## Mask Generation: \n",
    "\n",
    "Generates a binary mask based on the non-zero regions of an image, which is instrumental in blending.\n",
    "\n",
    "## Image Blending: \n",
    "\n",
    "The key function that ties everything together. It blends two images using Laplacian pyramids and a Gaussian mask. Masks are generated to identify overlapping regions and 'STRAIGHTCUT' is employed for blending.\n",
    "\n",
    "## Computational Complexity\n",
    "\n",
    "- Gaussian and Laplacian pyramid generation: $O(N \\log N)$ for $N$ pixels.\n",
    "- Blending and reconstruction: $O(N)$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageBlenderWithPyramids():\n",
    "    '''\n",
    "    Class for blending images using Gaussian and Laplacian pyramids.\n",
    "    It is specialized for the 'STRAIGHTCUT' blending strategy.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, pyramid_depth=6):\n",
    "        '''\n",
    "        Initialize with the depth of the pyramids.\n",
    "        '''\n",
    "        self.pyramid_depth = pyramid_depth\n",
    "\n",
    "    def getGaussianPyramid(self, image):\n",
    "        '''\n",
    "        Generate a Gaussian pyramid for a given image.\n",
    "        '''\n",
    "        pyramid = [image]\n",
    "        for _ in range(self.pyramid_depth - 1):\n",
    "            image = cv2.pyrDown(image)\n",
    "            pyramid.append(image)\n",
    "        return pyramid\n",
    "\n",
    "    def getLaplacianPyramid(self, image):\n",
    "        '''\n",
    "        Generate a Laplacian pyramid for a given image.\n",
    "        '''\n",
    "        pyramid = []\n",
    "        for _ in range(self.pyramid_depth - 1):\n",
    "            next_level_image = cv2.pyrDown(image)\n",
    "            upsampled_image = cv2.pyrUp(next_level_image, dstsize=(image.shape[1], image.shape[0]))\n",
    "            pyramid.append(image.astype(float) - upsampled_image.astype(float))\n",
    "            image = next_level_image\n",
    "        pyramid.append(image)\n",
    "        return pyramid\n",
    "\n",
    "    def getBlendingPyramid(self, laplacian_a, laplacian_b, gaussian_mask_pyramid):\n",
    "        '''\n",
    "        Create a blended Laplacian pyramid using two Laplacian pyramids and a Gaussian pyramid as mask.\n",
    "        '''\n",
    "        blended_pyramid = []\n",
    "        for i, mask in enumerate(gaussian_mask_pyramid):\n",
    "            triplet_mask = cv2.merge((mask, mask, mask))\n",
    "            blended_pyramid.append(laplacian_a[i] * triplet_mask + laplacian_b[i] * (1 - triplet_mask))\n",
    "        return blended_pyramid\n",
    "\n",
    "    def reconstructFromPyramid(self, laplacian_pyramid):\n",
    "        '''\n",
    "        Reconstruct an image from its Laplacian pyramid.\n",
    "        '''\n",
    "        reconstructed_image = laplacian_pyramid[-1]\n",
    "        for laplacian_level in reversed(laplacian_pyramid[:-1]):\n",
    "            reconstructed_image = cv2.pyrUp(reconstructed_image, dstsize=laplacian_level.shape[:2][::-1]).astype(float) + laplacian_level.astype(float)\n",
    "        return reconstructed_image\n",
    "\n",
    "    def generateMaskFromImage(self, image):\n",
    "        '''\n",
    "        Generate a mask based on the non-zero regions of the image.\n",
    "        '''\n",
    "        mask = np.all(image != 0, axis=2)\n",
    "        mask_image = np.zeros(image.shape[:2], dtype=float)\n",
    "        mask_image[mask] = 1.0\n",
    "        return mask_image\n",
    "\n",
    "    def blendImages(self, image1, image2):\n",
    "        '''\n",
    "        Blend two images using Laplacian pyramids and a Gaussian mask.\n",
    "        '''\n",
    "        laplacian1 = self.getLaplacianPyramid(image1)\n",
    "        laplacian2 = self.getLaplacianPyramid(image2)\n",
    "\n",
    "        mask1 = self.generateMaskFromImage(image1).astype(np.bool_)\n",
    "        mask2 = self.generateMaskFromImage(image2).astype(np.bool_)\n",
    "        \n",
    "        overlap_region = mask1 & mask2\n",
    "\n",
    "        y_coords, x_coords = np.where(overlap_region)\n",
    "        min_x, max_x = np.min(x_coords), np.max(x_coords)\n",
    "\n",
    "        final_mask = np.zeros(image1.shape[:2])\n",
    "        final_mask[:, :(min_x + max_x)//2] = 1.0\n",
    "\n",
    "        gaussian_mask_pyramid = self.getGaussianPyramid(final_mask)\n",
    "\n",
    "        blended_pyramid = self.getBlendingPyramid(laplacian1, laplacian2, gaussian_mask_pyramid)\n",
    "\n",
    "        blended_image = self.reconstructFromPyramid(blended_pyramid)\n",
    "\n",
    "        return blended_image, mask1, mask2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_and_save_images(src_idx, dest_idx, prev_homography):\n",
    "    '''\n",
    "    For a given pair of image indices, computes the best homography \n",
    "    matrix and saves the warped images to disk.\n",
    "\n",
    "    Args:\n",
    "        src_idx: Index of the source image in imagePaths list.\n",
    "        dest_idx: Index of the destination image in imagePaths list.\n",
    "        prev_homography: Previous cumulative homography matrix.\n",
    "\n",
    "    Returns:\n",
    "        new_cumulative_homography: Updated cumulative homography matrix.\n",
    "    '''\n",
    "    \n",
    "    # Initialize the destination warped image with zeros\n",
    "    warped_image = np.zeros((3000, 6000, 3), dtype=np.uint8)\n",
    "\n",
    "    # Read source and destination images\n",
    "    src_img = cv2.imread(imagePaths[src_idx])\n",
    "    dest_img = cv2.imread(imagePaths[dest_idx])\n",
    "\n",
    "    print(f'Original image size = {src_img.shape}')\n",
    "\n",
    "    # Resize images for performance and compatibility\n",
    "    resized_src_img = cv2.resize(src_img, shape)\n",
    "    resized_dest_img = cv2.resize(dest_img, shape)\n",
    "\n",
    "    # Detect features and perform matching between images\n",
    "    matches, src_pts, dest_pts = detectFeaturesAndMatch(resized_dest_img, resized_src_img)\n",
    "\n",
    "    # Compute the best homography using RANSAC\n",
    "    best_homography, _ = getBestHomographyRANSAC(matches, trials=trials, threshold=threshold)\n",
    "\n",
    "    # Update the cumulative homography matrix\n",
    "    new_cumulative_homography = np.dot(prev_homography, best_homography)\n",
    "\n",
    "    # Transform the source image and place it in the warped image\n",
    "    warpAndPlaceSourceImage(resized_dest_img, new_cumulative_homography, dest_img=warped_image, offset=offset)\n",
    "\n",
    "    # Save the warped image to disk\n",
    "    cv2.imwrite(f'outputs/scene{imageSet}/custom/warped_{dest_idx}.png', warped_image)\n",
    "\n",
    "    return new_cumulative_homography\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 1] Operation not permitted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/mihiragarwal/Desktop/Project Courses/Assignment - 2/Panorama-Stitching/assignment_2.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mihiragarwal/Desktop/Project%20Courses/Assignment%20-%202/Panorama-Stitching/assignment_2.ipynb#X66sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mihiragarwal/Desktop/Project%20Courses/Assignment%20-%202/Panorama-Stitching/assignment_2.ipynb#X66sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(os\u001b[39m.\u001b[39;49mgetcwd())\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 1] Operation not permitted"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING WITH CUSTOM IMPLEMENTATION\n",
      "STARTING WITH IMAGESET - 1\n",
      "['dataset/scene1/I11.JPG', 'dataset/scene1/I12.JPG', 'dataset/scene1/I13.JPG', 'dataset/scene1/I14.JPG']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stitch_and_save_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mihiragarwal/Desktop/Project Courses/Assignment - 2/assignment_2.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mihiragarwal/Desktop/Project%20Courses/Assignment%20-%202/assignment_2.ipynb#X21sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mif\u001b[39;00m(imageSet \u001b[39min\u001b[39;00m [\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m]):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mihiragarwal/Desktop/Project%20Courses/Assignment%20-%202/assignment_2.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     prevH \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39meye(\u001b[39m3\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mihiragarwal/Desktop/Project%20Courses/Assignment%20-%202/assignment_2.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     prevH \u001b[39m=\u001b[39m stitch_and_save_images(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, prevH)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mihiragarwal/Desktop/Project%20Courses/Assignment%20-%202/assignment_2.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     prevH \u001b[39m=\u001b[39m stitch_and_save_images(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, prevH)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mihiragarwal/Desktop/Project%20Courses/Assignment%20-%202/assignment_2.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     prevH \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39meye(\u001b[39m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stitch_and_save_images' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"STARTING WITH CUSTOM IMPLEMENTATION\")\n",
    "\n",
    "for imageSet in range(1,7):  \n",
    "    print(f\"STARTING WITH IMAGESET - {imageSet}\")\n",
    "    imagePaths = sorted(glob.glob(f'dataset/scene{imageSet}/*'))\n",
    "    print(imagePaths)\n",
    "    shape = (600, 400)\n",
    "    mid = len(imagePaths)//2\n",
    "    threshold = 2\n",
    "    trials = 3000\n",
    "    offset = [2300, 800]\n",
    "    if(imageSet in [1,2,3]):\n",
    "        prevH = np.eye(3)\n",
    "        prevH = stitch_and_save_images(2, 1, prevH)\n",
    "        prevH = stitch_and_save_images(1, 0, prevH)\n",
    "\n",
    "        prevH = np.eye(3)\n",
    "        prevH = stitch_and_save_images(2, 2, prevH)\n",
    "        prevH = stitch_and_save_images(2, 3, prevH)\n",
    "\n",
    "        print(\"warping complete for set\", imageSet)\n",
    "\n",
    "        # WARPING COMPLETE. BLENDING START\n",
    "        print(\"We are starting with blending\")\n",
    "        b = ImageBlenderWithPyramids()\n",
    "        finalImg = cv2.imread(f'outputs/scene{imageSet}/custom/warped_0.png')\n",
    "\n",
    "        for index in range(1, 4):\n",
    "            print('blending', index)\n",
    "            img2 = cv2.imread(f'outputs/scene{imageSet}/custom/warped_{index}.png')\n",
    "            finalImg, mask1truth, mask2truth = b.blendImages(finalImg, img2)\n",
    "            mask1truth = mask1truth + mask2truth\n",
    "        \n",
    "        cv2.imwrite(f'outputs/scene{imageSet}/custom/blended_image.png', finalImg)\n",
    "    else:\n",
    "        prevH = np.eye(3)\n",
    "        prevH = stitch_and_save_images(1, 0, prevH)\n",
    "        prevH = stitch_and_save_images(1, 1, prevH)\n",
    "\n",
    "        print(\"warping complete for set\", imageSet)\n",
    "\n",
    "        # WARPING COMPLETE. BLENDING START\n",
    "        print(\"We are starting with blending\")\n",
    "        b = ImageBlenderWithPyramids()\n",
    "        finalImg = cv2.imread(f'outputs/scene{imageSet}/custom/warped_0.png')\n",
    "\n",
    "        for index in range(1, 2):\n",
    "            print('blending', index)\n",
    "            img2 = cv2.imread(f'outputs/scene{imageSet}/custom/warped_{index}.png')\n",
    "            finalImg, mask1truth, mask2truth = b.blendImages(finalImg, img2)\n",
    "            mask1truth = mask1truth + mask2truth\n",
    "        \n",
    "        cv2.imwrite(f'outputs/scene{imageSet}/custom/blended_image.png', finalImg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPENCV IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Homography_opencv(correspondences, trials=1000, threshold=10, num_samples=4):\n",
    "        srcPoints = np.float32([point[0] for point in correspondences])\n",
    "        dstPoints = np.float32([point[1] for point in correspondences])\n",
    "        H, mask = cv2.findHomography(srcPoints, dstPoints, method=cv2.RANSAC, ransacReprojThreshold=threshold)\n",
    "        return H, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_and_save_images_opencv(src_idx, dest_idx, prev_homography):\n",
    "    '''\n",
    "    For a given pair of image indices, computes the best homography \n",
    "    matrix and saves the warped images to disk.\n",
    "\n",
    "    Args:\n",
    "        src_idx: Index of the source image in imagePaths list.\n",
    "        dest_idx: Index of the destination image in imagePaths list.\n",
    "        prev_homography: Previous cumulative homography matrix.\n",
    "\n",
    "    Returns:\n",
    "        new_cumulative_homography: Updated cumulative homography matrix.\n",
    "    '''\n",
    "    \n",
    "    # Initialize the destination warped image with zeros\n",
    "    warped_image = np.zeros((4192, 8192, 3), dtype=np.uint8)\n",
    "\n",
    "    # Read source and destination images\n",
    "    src_img = cv2.imread(imagePaths[src_idx])\n",
    "    dest_img = cv2.imread(imagePaths[dest_idx])\n",
    "\n",
    "    print(f'Original image size = {src_img.shape}')\n",
    "\n",
    "    # Resize images for performance and compatibility\n",
    "    resized_src_img = cv2.resize(src_img, shape)\n",
    "    resized_dest_img = cv2.resize(dest_img, shape)\n",
    "\n",
    "    # Detect features and perform matching between images\n",
    "    matches, src_pts, dest_pts = detectFeaturesAndMatch(resized_dest_img, resized_src_img)\n",
    "\n",
    "    # Compute the best homography using RANSAC\n",
    "    best_homography, _ = Homography_opencv(matches, trials=trials, threshold=threshold)\n",
    "\n",
    "    # Update the cumulative homography matrix\n",
    "    new_cumulative_homography = np.dot(prev_homography, best_homography)\n",
    "\n",
    "    # Transform the source image and place it in the warped image\n",
    "    warpAndPlaceSourceImage(resized_dest_img, new_cumulative_homography, dest_img=warped_image, offset=offset)\n",
    "\n",
    "    # Save the warped image to disk\n",
    "    cv2.imwrite(f'outputs/scene{imageSet}/opencv/warped_{dest_idx}.png', warped_image)\n",
    "\n",
    "    return new_cumulative_homography\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING WITH OPENCV IMPLEMENTATION\n",
      "STARTING WITH IMAGESET - 1\n",
      "Original image size = (2448, 3264, 3)\n",
      "Total number of matches: 1433\n",
      "Original image size = (2448, 3264, 3)\n",
      "Total number of matches: 1614\n",
      "Original image size = (2448, 3264, 3)\n",
      "Total number of matches: 1187\n",
      "Original image size = (2448, 3264, 3)\n",
      "Total number of matches: 1187\n",
      "warping complete for set 1\n",
      "We are starting with blending\n",
      "blending 1\n",
      "blending 2\n",
      "blending 3\n",
      "STARTING WITH IMAGESET - 2\n",
      "Original image size = (1329, 2000, 3)\n",
      "Total number of matches: 2133\n",
      "Original image size = (1329, 2000, 3)\n",
      "Total number of matches: 1633\n",
      "Original image size = (1329, 2000, 3)\n",
      "Total number of matches: 2626\n",
      "Original image size = (1329, 2000, 3)\n",
      "Total number of matches: 2792\n",
      "warping complete for set 2\n",
      "We are starting with blending\n",
      "blending 1\n",
      "blending 2\n",
      "blending 3\n",
      "STARTING WITH IMAGESET - 3\n",
      "Original image size = (2448, 3264, 3)\n",
      "Total number of matches: 1682\n",
      "Original image size = (2448, 3264, 3)\n",
      "Total number of matches: 1221\n",
      "Original image size = (2448, 3264, 3)\n",
      "Total number of matches: 1614\n",
      "Original image size = (2448, 3264, 3)\n",
      "Total number of matches: 1433\n",
      "warping complete for set 3\n",
      "We are starting with blending\n",
      "blending 1\n",
      "blending 2\n",
      "blending 3\n",
      "STARTING WITH IMAGESET - 4\n",
      "Original image size = (360, 640, 3)\n",
      "Total number of matches: 3277\n",
      "Original image size = (360, 640, 3)\n",
      "Total number of matches: 3563\n",
      "warping complete for set 4\n",
      "We are starting with blending\n",
      "blending 1\n",
      "STARTING WITH IMAGESET - 5\n",
      "Original image size = (360, 640, 3)\n",
      "Total number of matches: 1693\n",
      "Original image size = (360, 640, 3)\n",
      "Total number of matches: 1647\n",
      "warping complete for set 5\n",
      "We are starting with blending\n",
      "blending 1\n",
      "STARTING WITH IMAGESET - 6\n",
      "Original image size = (360, 640, 3)\n",
      "Total number of matches: 2081\n",
      "Original image size = (360, 640, 3)\n",
      "Total number of matches: 2067\n",
      "warping complete for set 6\n",
      "We are starting with blending\n",
      "blending 1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"STARTING WITH OPENCV IMPLEMENTATION\")\n",
    "\n",
    "for imageSet in range(1,7):  \n",
    "    print(f\"STARTING WITH IMAGESET - {imageSet}\")\n",
    "    imagePaths = sorted(glob.glob(f'dataset/scene{imageSet}/*'))\n",
    "    shape = (600, 400)\n",
    "    mid = len(imagePaths)//2\n",
    "    threshold = 2\n",
    "    trials = 5000\n",
    "    offset = [2300, 800]\n",
    "    if(imageSet in [1,2,3]):\n",
    "        prevH = np.eye(3)\n",
    "        prevH = stitch_and_save_images_opencv(2, 1, prevH)\n",
    "        prevH = stitch_and_save_images_opencv(1, 0, prevH)\n",
    "\n",
    "        prevH = np.eye(3)\n",
    "        prevH = stitch_and_save_images_opencv(2, 2, prevH)\n",
    "        prevH = stitch_and_save_images_opencv(2, 3, prevH)\n",
    "\n",
    "        print(\"warping complete for set\", imageSet)\n",
    "\n",
    "        # WARPING COMPLETE. BLENDING START\n",
    "        print(\"We are starting with blending\")\n",
    "        b = ImageBlenderWithPyramids()\n",
    "        finalImg = cv2.imread(f'outputs/scene{imageSet}/opencv/warped_0.png')\n",
    "\n",
    "        for index in range(1, 4):\n",
    "            print('blending', index)\n",
    "            img2 = cv2.imread(f'outputs/scene{imageSet}/opencv/warped_{index}.png')\n",
    "            finalImg, mask1truth, mask2truth = b.blendImages(finalImg, img2)\n",
    "            mask1truth = mask1truth + mask2truth\n",
    "        \n",
    "        cv2.imwrite(f'outputs/scene{imageSet}/opencv/blended_image.png', finalImg)\n",
    "    else:\n",
    "        prevH = np.eye(3)\n",
    "        prevH = stitch_and_save_images_opencv(1, 0, prevH)\n",
    "        prevH = stitch_and_save_images_opencv(1, 1, prevH)\n",
    "\n",
    "        print(\"warping complete for set\", imageSet)\n",
    "\n",
    "        # WARPING COMPLETE. BLENDING START\n",
    "        print(\"We are starting with blending\")\n",
    "        b = ImageBlenderWithPyramids()\n",
    "        finalImg = cv2.imread(f'outputs/scene{imageSet}/opencv/warped_0.png')\n",
    "\n",
    "        for index in range(1, 2):\n",
    "            print('blending', index)\n",
    "            img2 = cv2.imread(f'outputs/scene{imageSet}/opencv/warped_{index}.png')\n",
    "            finalImg, mask1truth, mask2truth = b.blendImages(finalImg, img2)\n",
    "            mask1truth = mask1truth + mask2truth\n",
    "        \n",
    "        cv2.imwrite(f'outputs/scene{imageSet}/opencv/blended_image.png', finalImg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
